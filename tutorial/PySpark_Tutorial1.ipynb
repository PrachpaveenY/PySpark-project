{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** run 1\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** run 2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[*]\")\n",
    "spark = SparkSession(sparkContext=sc)\\\n",
    "        .builder\\\n",
    "        .appName(\"PySpark\")\\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# ต้อง Run ส่วนนี้ทุกครั้งเพื่อให้ข้อมูลถูกเก็บที่ SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_a = spark.read.json(\"customer.json\")\n",
    "json_b = spark.read.load(\"people.json\", format=\"json\")\n",
    "\n",
    "parquet_a = spark.read.load(\"users.parquet\")\n",
    "\n",
    "txt_a = spark.read.text(\"people.txt\")\n",
    "\n",
    "csv_a = spark.read.csv('mtcars.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## header=True, inferSchema=True\n",
    "csv_a = spark.read.csv('HR01.csv', header=True, inferSchema=True)\n",
    "\n",
    "#  ข้อมูลจะถูกเก็บไว้ในหน่วยความจำของ Spark ในขณะประมวลผล \n",
    "#  หากต้องการเก็บข้อมูลไว้ถาวร สามารถบันทึกข้อมูลลงดิสก์ได้โดยใช้คำสั่ง csv_a.write.csv(\"path/to/output.csv\") ( ข้อมูลจะถูกบันทึกลงไฟล์ CSV ที่ระบุ ใน path/to/output.csv )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "|Employee ID|      Name| Department|         Position|Salary|Bonus|Start Date|  End Date|  Resignation Reason|\n",
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "|       7780|qiqhdlnhjf|      Sales|    Sales Manager| 95195|44918|2023-01-01|2023-12-31|Taking a break fr...|\n",
      "|       7484|gjfermnzzi|    Finance|       Accountant| 74028|26978|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       1922|pvaarwbvtu|Engineering|    Sales Manager| 94516| 5975|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       6859|qsiyfoytqq|    Finance|Marketing Manager| 57882|49312|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       6211|dsvjwwhzyw|  Marketing|       Accountant| 89673|27118|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       9226|dbcldodcnv|Engineering|    Sales Manager| 97405|15871|2023-01-01|2023-12-31|     Found a new job|\n",
      "|        755|ulmyvioojm|    Finance|Software Engineer| 79951|35278|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       2819|eyhxfsliiz|    Finance|Marketing Manager| 90018|47049|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       8329|vnyrnuzgtd|    Finance|Marketing Manager| 69310|33517|2023-01-01|2023-12-31|Taking a break fr...|\n",
      "|       8567|rzvwjnvraj|    Finance|    Sales Manager| 87425|14494|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       7105|ridlzjwdpe|Engineering|       Accountant| 55608|17035|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       7829|evgsgmgnil|Engineering|Software Engineer| 65188|29477|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|        482|vgbmdcfglz|  Marketing|    Sales Manager| 97397| 4103|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       9552|qbbptozpnb|Engineering|Marketing Manager| 76681|25480|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       5830|mmxsiycxvv|      Sales|Marketing Manager| 81934| 7997|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       5178|uhddrsgpex|      Sales|Software Engineer| 97916| 1644|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       5695|obvvfthjax|Engineering|Software Engineer| 86205|18429|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       9852|cqamtaxqae|    Finance|Software Engineer| 75518|15551|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       9331|pnjmmqjcow|      Sales|Marketing Manager| 75197|20356|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       1576|hkumvqvynx|    Finance|       Accountant| 50121|41951|2023-01-01|2023-12-31|Moving to a new city|\n",
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|Employee ID|      Name|\n",
      "+-----------+----------+\n",
      "|       7780|qiqhdlnhjf|\n",
      "|       7484|gjfermnzzi|\n",
      "|       1922|pvaarwbvtu|\n",
      "|       6859|qsiyfoytqq|\n",
      "|       6211|dsvjwwhzyw|\n",
      "|       9226|dbcldodcnv|\n",
      "|        755|ulmyvioojm|\n",
      "|       2819|eyhxfsliiz|\n",
      "|       8329|vnyrnuzgtd|\n",
      "|       8567|rzvwjnvraj|\n",
      "|       7105|ridlzjwdpe|\n",
      "|       7829|evgsgmgnil|\n",
      "|        482|vgbmdcfglz|\n",
      "|       9552|qbbptozpnb|\n",
      "|       5830|mmxsiycxvv|\n",
      "|       5178|uhddrsgpex|\n",
      "|       5695|obvvfthjax|\n",
      "|       9852|cqamtaxqae|\n",
      "|       9331|pnjmmqjcow|\n",
      "|       1576|hkumvqvynx|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## เรียกดูข้อมูลเฉพาะคอลัมน์ Employee ID และ Name\n",
    "csv_a.select(\"Employee ID\", \"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "|Employee ID|      Name| Department|         Position|Salary|Bonus|Start Date|  End Date|  Resignation Reason|\n",
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "|       7780|qiqhdlnhjf|      Sales|    Sales Manager| 95195|44918|2023-01-01|2023-12-31|Taking a break fr...|\n",
      "|       7484|gjfermnzzi|    Finance|       Accountant| 74028|26978|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       1922|pvaarwbvtu|Engineering|    Sales Manager| 94516| 5975|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       6211|dsvjwwhzyw|  Marketing|       Accountant| 89673|27118|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       9226|dbcldodcnv|Engineering|    Sales Manager| 97405|15871|2023-01-01|2023-12-31|     Found a new job|\n",
      "|        755|ulmyvioojm|    Finance|Software Engineer| 79951|35278|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       2819|eyhxfsliiz|    Finance|Marketing Manager| 90018|47049|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       8567|rzvwjnvraj|    Finance|    Sales Manager| 87425|14494|2023-01-01|2023-12-31|     Found a new job|\n",
      "|        482|vgbmdcfglz|  Marketing|    Sales Manager| 97397| 4103|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       9552|qbbptozpnb|Engineering|Marketing Manager| 76681|25480|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       5830|mmxsiycxvv|      Sales|Marketing Manager| 81934| 7997|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       5178|uhddrsgpex|      Sales|Software Engineer| 97916| 1644|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       5695|obvvfthjax|Engineering|Software Engineer| 86205|18429|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       9852|cqamtaxqae|    Finance|Software Engineer| 75518|15551|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       9331|pnjmmqjcow|      Sales|Marketing Manager| 75197|20356|2023-01-01|2023-12-31|     Found a new job|\n",
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## เรียกดูข้อมูลของพนักงานที่มีเงินเดือนมากกว่า 100,000 บาท\n",
    "csv_a.filter(csv_a[\"Salary\"] > 70000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_a = spark.read.csv('HR01.csv', header=False, inferSchema=False)\n",
    "\n",
    "## กำหนด schema ของ DataFrame ด้วยตัวเอง (กรณีไฟล์ไม่มี header row อยู่ด้านบน)\n",
    "schema = StructType([\n",
    "    StructField(\"Employee ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Salary\", DoubleType(), True),\n",
    "    StructField(\"Bonus\", DoubleType(), True),\n",
    "    StructField(\"Start Date\", DateType(), True),\n",
    "    StructField(\"End Date\", DateType(), True),\n",
    "    StructField(\"Resignation Reason\", StringType(), True),\n",
    "])\n",
    "\n",
    "csv_a.schema = schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## เช็ค Data type\n",
    "csv_a.dtypes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## แสดงเนื้อหา\n",
    "csv_a.show() \n",
    "\n",
    "## แสดง n แถวแรก\n",
    "csv_a.head(n)\n",
    "\n",
    "## แสดงแถวแรกเท่านั้น\n",
    "csv_a.first()\n",
    "\n",
    "## แสดง n แถวแรกเท่านั้น โดยแสดงออกมาเป็น list ของแถว\n",
    "csv_a.take(2) \n",
    "\n",
    "## แสดง statistics (count, mean, stddev, min, max)\n",
    "csv_a.describe().show()\n",
    "csv_a.describe(['age']).show()\n",
    "\n",
    "## แสดง column  -  อยากรู้ว่าคอลัมน์มีอะไรบ้าง โดยที่จะแสดงชื่อคอลัมน์ออกมาเป็น list\n",
    "csv_a.columns \n",
    "\n",
    "## นับจำนวนแถว  -  อยากรู้ว่าดาต้าเฟรมของเรามีจำนวนแถวทั้งหมดกี่แถว\n",
    "csv_a.count() \n",
    "\n",
    "## นับจำนวนแถวที่ unique\n",
    "csv_a.distinct().count() \n",
    "\n",
    "## การดรอป ค่าซ้ำ  -  เมื่อมีการใส่ค่าผิด อย่างเช่น ใส่อายุผิด โดยที่ชื่อและ ความสูงถูก ทำให้มีแถวที่ซ้ำกันสองแถว เราสามารถดรอปแถวที่ซ้ำได้\n",
    "csv_a.dropDuplicates()\n",
    "csv_a.dropDuplicates().show()\n",
    "csv_a.dropDuplicates(['name', 'height']).show()\n",
    "# ปล. dropDuplicates() ให้ผลเหมือนกับ drop_duplicates()\n",
    "\n",
    "## การดรอป NA  -  การดรอป null\n",
    "csv_a.dropna()         # Method 1\n",
    "csv_a.na.drop()        # Method 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### การเลือกข้อมูลนั้นทำได้หลายวิธี โดยใช้ Queries ได้ดังต่อไปนี้ ###\n",
    "\n",
    "## การเลือกข้อมูลโดยใช้ Select\n",
    "# เรียกดาต้าจากดาต้าเฟรมแบบเฉพาะเจาะจงสามารถทำได้โดยใช้คำสั่ง select() ซึ่งเป็นคำสั่งที่สามารถเลือกชื่อคอลัมน์ที่เราต้องการจะดูได้ นอกจากนี้ยังจัดการดาต้าเพิ่มได้\n",
    "csv_a.select(\"name\").show()                            # เลือกคอลัมน์ name อย่างเดียว\n",
    "csv_a.select(csv_a['name'], csv_a['age'] + 1).show()         # เลือกทั้งสองคอลัมน์ โดยบวกอายุเพิ่มให้ทุกคน 1 ปี\n",
    "\n",
    "## การเลือกข้อมูลโดยใช้ When\n",
    "# ไว้ใช้เลือกในกรณีที่มี condition เข้ามาด้วย\n",
    "# แต่ถ้าเราไม่ใส่ otherwise ผลจะแสดงแค่คนที่มีอายุมากกว่า 20 คือแถวแรกแถวเดียว\n",
    "# เรายังสามารถใช้ when ซ้อน when ในกรณีที่มีหลาย conditions\n",
    "from pyspark.sql import functions as F\n",
    "csv_a.select(csv_a.name, F.when(csv_a.age > 20, 1).otherwise(0)).show()      # อยากรู้ว่าใครอายุมากกว่า 20 บ้าง ให้แสดงเป็น 1 และให้แสดงเป็น 0 โดยใช้ otherwise\n",
    "csv_a.select(csv_a.name, F.when(csv_a.age > 18, 1).when(df.age < 40, -1).otherwise(0)).show()\n",
    "\n",
    "## การเลือกข้อมูลโดยใช้ Like\n",
    "# Like เป็นคำสั่งเมื่อเราต้องการเทียบ หรือจับคู่เหมือน ยกตัวอย่างเช่นการหาคนชื่อซ้ำ หรือหาคนนามสกุลเดียวกัน\n",
    "csv_a.select(\"firstName\", csv_a.lastName.like(\"Smith\")).show()        # ให้แสดง ชื่อ แล้วก็นามสกุล สำหรับคนที่มีนามสกุล Smith\n",
    "\n",
    "## การเลือกข้อมูลโดยใช้ Startswith หรือ Endwith\n",
    "# ใช้ในกรณีที่ต้องการจับคู่คำขึ้นต้นหรือ คำลงท้าย\n",
    "csv_a.select(\"firstName\", csv_a.lastName.startswith(\"Sm\")).show()       # การหาคนที่มีนามสกุลขึ้นต้นด้วย ‘Sm’\n",
    "csv_a.select(csv_a.lastName.endswith(\"th\")).show()                      # หาคนที่มีนามสกุลลงท้ายด้วย ‘th’ \n",
    "\n",
    "## การใช้ Substring\n",
    "# เป็นคำสั่งที่ไว้ใช้เลือกดาต้าในส่วนย่อยเข้าไปอีก โดยเลือกคอลัมน์ที่อยากให้แสดง ตามด้วยฟังชั่น substr แล้วระบุจำนวนตัวอักษรว่าให้เริ่มจากตัวที่เท่าไหร่ จบที่ตัวที่เท่าไหร่ โดยนับจากตัวแรก\n",
    "csv_a.select(csv_a.firstName.substr(1, 3).alias(\"name\")).collect()[Row(name='And'), Row(name='Mic')]        # จากตัวที่หนึ่งถึงตัวที่สาม\n",
    "# alias = มันสามารถเอาไว้เปลี่ยนชื่อคอลัมน์ตอนแสดงออกมาได้ จาก first name เป็น name \n",
    "\n",
    "## การใช้ Between\n",
    "# เป็นการเลือกข้อมูลในช่วงนับตั้งแต่ขอบเขตด้านล่าง ขึ้นไปจนถึงขอบเขตด้านบน และจะแสดงผลออกมาเป็น boolean หรือว่า true/false\n",
    "csv_a.select(csv_a.name, csv_a.age.between(2, 4)).show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### วิธีการ เพิ่ม, ลบ, อัพเดท คอลัมน์ใน DataFrame ###\n",
    "# สามารถจัดการ เพิ่ม ลด เปลี่ยนชื่อ คอลัมน์ได้ดังต่อไปนี้\n",
    "\n",
    "## การเพิ่มคอลัมน์\n",
    "# ใช้ withColumn ตามด้วยชื่อคอลัมน์ที่เราจะตั้ง แล้วตามด้วยค่าในคอลัมน์นั้น, สามารถใช้ค่าจากคอลัมน์ที่มีอยู่แล้วได้ ขึ้นอยู่กับลักษณะข้อมูล\n",
    "csv_a.withColumn('age2', csv_a.age + 2)                     # เป็นการสร้างคอลัมน์ชื่อ age2 โดยการเพิ่มอายุจากคอลัมน์ age ไปอีก 2 ปี\n",
    "csv_a.withColumn('city',csv_a.address.city) \\\n",
    " .withColumn('postalCode',csv_a.address.postalCode) \\\n",
    " .withColumn('state',csv_a.address.state)                   # จะเป็นการสร้างคอลัมน์ใหม่ให้กับ city, postal code และ state โดยเลือกมาจากแต่ละส่วนของ address\n",
    "\n",
    "## การลบคอลัมน์\n",
    "# เมื่อต้องการลบคอลัมน์ที่ไม่ต้องการ ใช้ drop แล้วตามด้วยชื่อคอลัมน์\n",
    "csv_a.drop(\"address\", \"phoneNumber\")                    # การดรอป 2 คอลัมน์ address และ phone number\n",
    "csv_a.drop(csv_a.address).drop(csv_a.phoneNumber)       # เราสามารถทำการ ดรอปซ้อนดรอปได้ดังนี้\n",
    "\n",
    "## การอัพเดทคอลัมน์\n",
    "# อยากเปลี่ยนชื่อคอลัมน์ สามารถทำได้โดยใช้ withColumnRenamed ระบุชื่อคอลัมน์ที่ต้องการจะเปลี่ยนตามด้วยชื่อใหม่\n",
    "csv_a = csv_a.withColumnRenamed('telePhoneNumber', 'phoneNumber')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### การใช้ Operation จัดกลุ่ม กรอง เรียง ข้อมูล\n",
    "# วิธีการใช้ operation ต่างๆโดยเราสามารถจัดกลุ่ม กรองดาต้าตามเงื่อนไข หรือแม้แต่เรียงลำดับได้\n",
    "\n",
    "## การจัดกลุ่มโดย GroupBy\n",
    "# เราสามารถจัดกลุ่มดาต้า แล้วหาจำนวนในแต่ละกลุ่ม หรือค่าเฉลี่ยได้\n",
    "csv_a.groupBy(\"age\").count().show()         # เราอยากรู้ว่าคนอายุเท่านี้ มีทั้งหมดกี่คน ก็ทำได้โดยให้ groupBy อายุ แล้วนับจำนวนด้วย count\n",
    "\n",
    "## การจัดกลุ่มโดย GroupBy\n",
    "# เราสามารถกำหนด condition ได้ \n",
    "csv_a.filter(csv_a['age'] > 21).show()      # การเลือกคนทั้งหมดที่มีอายุมากกว่า น้อยกว่า หรือเท่ากับ ตัวเลขหนึ่ง โดยจะแสดงดาต้าเฟรมออกมาเฉพาะแถวที่เข้า condition นั้น\n",
    "\n",
    "## การเรียงลำดับข้อมูลโดยใช้ Sort\n",
    "# การเรียงลำดับข้อมูล จากมากไปน้อย (descending) หรือ จากน้อยไปมาก (ascending)\n",
    "csv_a.sort(csv_a.age.desc())                               # วิธีที่ 1 จากมากไปน้อย\n",
    "csv_a.sort(\"age\", ascending=False)                         # วิธีที่ 2 จากมากไปน้อย โดยให้น้อยไปมาก เป็น false\n",
    "csv_a.orderBy([\"age\",\"city\"],ascending=[0,1])              # วิธีที่ 3 จากมากไปน้อยโดย age, จากน้อยไปมากโดย city\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### การใช้ Operation จัดกลุ่ม กรอง เรียง ข้อมูล\n",
    "\n",
    "## การเปลี่ยนชนิดของข้อมูล\n",
    "# เราสามารถเล่นกับข้อมูลโดยเปลี่ยน ชนิดของดาต้าไปมาได้ ตั้งแต่เปลี่ยนดาต้าเฟรมเป็น RDD หรือ ให้อยู่ในรูปแบบ Pandas นอกจากเปลี่ยนไปแล้วก็ยังเปลี่ยนกลับได้ด้วย\n",
    "rdd1 = csv_a.rdd                                   # เปลี่ยน dataframe เป็น RDD\n",
    "csv_a.toJSON().first()                             # เปลี่ยน dataframe เป็น string RDD\n",
    "csv_a.toPandas()                                   # ทำให้ spark dataframe อยู่ในรูปแบบ pandas dataframe\n",
    "csv_a = spark.createDataFrame(pandas_df)           # ทำให้ pandas dataframe อยู่ในรูปแบบ spark dataframe\n",
    "\n",
    "## การเซฟไฟล์เป็น Parquet files\n",
    "# ถ้าเราอยากเซฟไฟล์เป็น parquet สามารถทำได้โดยใช้ write.save แล้วตามด้วยชื่อ\n",
    "csv_a.write.save(\"newFile.parquet\")\n",
    "\n",
    "## การเซฟไฟล์เป็น JSON\n",
    "# ในกรณีของ json สามารถทำได้โดยใช้ write.save แล้วตามด้วยชื่อ จากนั้นระบุ format เข้าไปด้วย\n",
    "csv_a.write.save(\"newFile.json\",format=\"json\")\n",
    "\n",
    "## หยุดใช้งาน Spark\n",
    "# ในกรณีที่อยากหยุดการทำงานของ Spark\n",
    "spark.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Bonus: integer (nullable = true)\n",
      " |-- Start Date: date (nullable = true)\n",
      " |-- End Date: date (nullable = true)\n",
      " |-- Resignation Reason: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## แสดง schema\n",
    "csv_a.printSchema()\n",
    "\n",
    "# แสดง schema ดูว่าคอลัมน์มีอะไรบ้าง ชนิดอะไร "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## แสดง statistics (count, mean, stddev, min, max)\n",
    "csv_a.describe().show() \n",
    "\n",
    "## ในกรณีที่ต้องการให้แสดงเฉพาะคอลัมน์ก็ใส่ชื่อเข้าไปได้\n",
    "csv_a.describe(['age']).show()\n",
    "\n",
    "# อยากรู้สถิติของข้อมูลว่าแต่ละคอลัมน์เป็นอย่างไร ทำได้โดยใช้ describe() ตามด้วย show() เพื่อแสดงข้อมูล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## การดรอป ค่าซ้ำ  -  เมื่อมีการใส่ค่าผิด อย่างเช่น ใส่อายุผิด โดยที่ชื่อและ ความสูงถูก ทำให้มีแถวที่ซ้ำกันสองแถว เราสามารถดรอปแถวที่ซ้ำได้\n",
    "df.dropDuplicates()\n",
    "\n",
    "## พอเราดรอปค่าซ้ำ\n",
    "df.dropDuplicates().show()\n",
    "\n",
    "## ถ้สต้องการดรอปโดบเจาะจงคอลัมน์\n",
    "df.dropDuplicates(['name', 'height']).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
