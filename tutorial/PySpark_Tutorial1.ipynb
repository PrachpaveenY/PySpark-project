{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** run 1\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** run 2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[*]\")\n",
    "spark = SparkSession(sparkContext=sc)\\\n",
    "        .builder\\\n",
    "        .appName(\"PySpark\")\\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# ต้อง Run ส่วนนี้ทุกครั้งเพื่อให้ข้อมูลถูกเก็บที่ SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_a = spark.read.json(\"customer.json\")\n",
    "json_b = spark.read.load(\"people.json\", format=\"json\")\n",
    "\n",
    "parquet_a = spark.read.load(\"users.parquet\")\n",
    "\n",
    "txt_a = spark.read.text(\"people.txt\")\n",
    "\n",
    "csv_a = spark.read.csv('mtcars.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## header=True, inferSchema=True\n",
    "csv_a = spark.read.csv('HR01.csv', header=True, inferSchema=True)\n",
    "\n",
    "#  ข้อมูลจะถูกเก็บไว้ในหน่วยความจำของ Spark ในขณะประมวลผล \n",
    "#  หากต้องการเก็บข้อมูลไว้ถาวร สามารถบันทึกข้อมูลลงดิสก์ได้โดยใช้คำสั่ง csv_a.write.csv(\"path/to/output.csv\") ( ข้อมูลจะถูกบันทึกลงไฟล์ CSV ที่ระบุ ใน path/to/output.csv )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "|Employee ID|      Name| Department|         Position|Salary|Bonus|Start Date|  End Date|  Resignation Reason|\n",
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "|       7780|qiqhdlnhjf|      Sales|    Sales Manager| 95195|44918|2023-01-01|2023-12-31|Taking a break fr...|\n",
      "|       7484|gjfermnzzi|    Finance|       Accountant| 74028|26978|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       1922|pvaarwbvtu|Engineering|    Sales Manager| 94516| 5975|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       6859|qsiyfoytqq|    Finance|Marketing Manager| 57882|49312|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       6211|dsvjwwhzyw|  Marketing|       Accountant| 89673|27118|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       9226|dbcldodcnv|Engineering|    Sales Manager| 97405|15871|2023-01-01|2023-12-31|     Found a new job|\n",
      "|        755|ulmyvioojm|    Finance|Software Engineer| 79951|35278|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       2819|eyhxfsliiz|    Finance|Marketing Manager| 90018|47049|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       8329|vnyrnuzgtd|    Finance|Marketing Manager| 69310|33517|2023-01-01|2023-12-31|Taking a break fr...|\n",
      "|       8567|rzvwjnvraj|    Finance|    Sales Manager| 87425|14494|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       7105|ridlzjwdpe|Engineering|       Accountant| 55608|17035|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       7829|evgsgmgnil|Engineering|Software Engineer| 65188|29477|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|        482|vgbmdcfglz|  Marketing|    Sales Manager| 97397| 4103|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       9552|qbbptozpnb|Engineering|Marketing Manager| 76681|25480|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       5830|mmxsiycxvv|      Sales|Marketing Manager| 81934| 7997|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       5178|uhddrsgpex|      Sales|Software Engineer| 97916| 1644|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       5695|obvvfthjax|Engineering|Software Engineer| 86205|18429|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       9852|cqamtaxqae|    Finance|Software Engineer| 75518|15551|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       9331|pnjmmqjcow|      Sales|Marketing Manager| 75197|20356|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       1576|hkumvqvynx|    Finance|       Accountant| 50121|41951|2023-01-01|2023-12-31|Moving to a new city|\n",
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|Employee ID|      Name|\n",
      "+-----------+----------+\n",
      "|       7780|qiqhdlnhjf|\n",
      "|       7484|gjfermnzzi|\n",
      "|       1922|pvaarwbvtu|\n",
      "|       6859|qsiyfoytqq|\n",
      "|       6211|dsvjwwhzyw|\n",
      "|       9226|dbcldodcnv|\n",
      "|        755|ulmyvioojm|\n",
      "|       2819|eyhxfsliiz|\n",
      "|       8329|vnyrnuzgtd|\n",
      "|       8567|rzvwjnvraj|\n",
      "|       7105|ridlzjwdpe|\n",
      "|       7829|evgsgmgnil|\n",
      "|        482|vgbmdcfglz|\n",
      "|       9552|qbbptozpnb|\n",
      "|       5830|mmxsiycxvv|\n",
      "|       5178|uhddrsgpex|\n",
      "|       5695|obvvfthjax|\n",
      "|       9852|cqamtaxqae|\n",
      "|       9331|pnjmmqjcow|\n",
      "|       1576|hkumvqvynx|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## เรียกดูข้อมูลเฉพาะคอลัมน์ Employee ID และ Name\n",
    "csv_a.select(\"Employee ID\", \"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "|Employee ID|      Name| Department|         Position|Salary|Bonus|Start Date|  End Date|  Resignation Reason|\n",
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "|       7780|qiqhdlnhjf|      Sales|    Sales Manager| 95195|44918|2023-01-01|2023-12-31|Taking a break fr...|\n",
      "|       7484|gjfermnzzi|    Finance|       Accountant| 74028|26978|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       1922|pvaarwbvtu|Engineering|    Sales Manager| 94516| 5975|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       6211|dsvjwwhzyw|  Marketing|       Accountant| 89673|27118|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       9226|dbcldodcnv|Engineering|    Sales Manager| 97405|15871|2023-01-01|2023-12-31|     Found a new job|\n",
      "|        755|ulmyvioojm|    Finance|Software Engineer| 79951|35278|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       2819|eyhxfsliiz|    Finance|Marketing Manager| 90018|47049|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       8567|rzvwjnvraj|    Finance|    Sales Manager| 87425|14494|2023-01-01|2023-12-31|     Found a new job|\n",
      "|        482|vgbmdcfglz|  Marketing|    Sales Manager| 97397| 4103|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       9552|qbbptozpnb|Engineering|Marketing Manager| 76681|25480|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       5830|mmxsiycxvv|      Sales|Marketing Manager| 81934| 7997|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       5178|uhddrsgpex|      Sales|Software Engineer| 97916| 1644|2023-01-01|2023-12-31|     Found a new job|\n",
      "|       5695|obvvfthjax|Engineering|Software Engineer| 86205|18429|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       9852|cqamtaxqae|    Finance|Software Engineer| 75518|15551|2023-01-01|2023-12-31|Moving to a new city|\n",
      "|       9331|pnjmmqjcow|      Sales|Marketing Manager| 75197|20356|2023-01-01|2023-12-31|     Found a new job|\n",
      "+-----------+----------+-----------+-----------------+------+-----+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## เรียกดูข้อมูลของพนักงานที่มีเงินเดือนมากกว่า 100,000 บาท\n",
    "csv_a.filter(csv_a[\"Salary\"] > 70000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_a = spark.read.csv('HR01.csv', header=False, inferSchema=False)\n",
    "\n",
    "## กำหนด schema ของ DataFrame ด้วยตัวเอง (กรณีไฟล์ไม่มี header row อยู่ด้านบน)\n",
    "schema = StructType([\n",
    "    StructField(\"Employee ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Salary\", DoubleType(), True),\n",
    "    StructField(\"Bonus\", DoubleType(), True),\n",
    "    StructField(\"Start Date\", DateType(), True),\n",
    "    StructField(\"End Date\", DateType(), True),\n",
    "    StructField(\"Resignation Reason\", StringType(), True),\n",
    "])\n",
    "\n",
    "csv_a.schema = schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## เช็ค Data type\n",
    "csv_a.dtypes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## แสดงเนื้อหา\n",
    "csv_a.show() \n",
    "\n",
    "## แสดง n แถวแรก\n",
    "csv_a.head(n)\n",
    "\n",
    "## แสดงแถวแรกเท่านั้น\n",
    "csv_a.first()\n",
    "\n",
    "## แสดง n แถวแรกเท่านั้น โดยแสดงออกมาเป็น list ของแถว\n",
    "csv_a.take(2) \n",
    "\n",
    "## แสดง statistics (count, mean, stddev, min, max)\n",
    "csv_a.describe().show()\n",
    "csv_a.describe(['age']).show()\n",
    "\n",
    "## แสดง column  -  อยากรู้ว่าคอลัมน์มีอะไรบ้าง โดยที่จะแสดงชื่อคอลัมน์ออกมาเป็น list\n",
    "csv_a.columns \n",
    "\n",
    "## นับจำนวนแถว  -  อยากรู้ว่าดาต้าเฟรมของเรามีจำนวนแถวทั้งหมดกี่แถว\n",
    "csv_a.count() \n",
    "\n",
    "## นับจำนวนแถวที่ unique\n",
    "csv_a.distinct().count() \n",
    "\n",
    "## การดรอป ค่าซ้ำ  -  เมื่อมีการใส่ค่าผิด อย่างเช่น ใส่อายุผิด โดยที่ชื่อและ ความสูงถูก ทำให้มีแถวที่ซ้ำกันสองแถว เราสามารถดรอปแถวที่ซ้ำได้\n",
    "csv_a.dropDuplicates()\n",
    "csv_a.dropDuplicates().show()\n",
    "csv_a.dropDuplicates(['name', 'height']).show()\n",
    "# ปล. dropDuplicates() ให้ผลเหมือนกับ drop_duplicates()\n",
    "\n",
    "## การดรอป NA  -  การดรอป null\n",
    "csv_a.dropna()         # Method 1\n",
    "csv_a.na.drop()        # Method 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### การเลือกข้อมูลนั้นทำได้หลายวิธี โดยใช้ Queries ได้ดังต่อไปนี้ ###\n",
    "\n",
    "## การเลือกข้อมูลโดยใช้ Select\n",
    "# เรียกดาต้าจากดาต้าเฟรมแบบเฉพาะเจาะจงสามารถทำได้โดยใช้คำสั่ง select() ซึ่งเป็นคำสั่งที่สามารถเลือกชื่อคอลัมน์ที่เราต้องการจะดูได้ นอกจากนี้ยังจัดการดาต้าเพิ่มได้\n",
    "csv_a.select(\"name\").show()                            # เลือกคอลัมน์ name อย่างเดียว\n",
    "csv_a.select(csv_a['name'], csv_a['age'] + 1).show()         # เลือกทั้งสองคอลัมน์ โดยบวกอายุเพิ่มให้ทุกคน 1 ปี\n",
    "\n",
    "## การเลือกข้อมูลโดยใช้ When\n",
    "# ไว้ใช้เลือกในกรณีที่มี condition เข้ามาด้วย\n",
    "# แต่ถ้าเราไม่ใส่ otherwise ผลจะแสดงแค่คนที่มีอายุมากกว่า 20 คือแถวแรกแถวเดียว\n",
    "# เรายังสามารถใช้ when ซ้อน when ในกรณีที่มีหลาย conditions\n",
    "from pyspark.sql import functions as F\n",
    "csv_a.select(csv_a.name, F.when(csv_a.age > 20, 1).otherwise(0)).show()      # อยากรู้ว่าใครอายุมากกว่า 20 บ้าง ให้แสดงเป็น 1 และให้แสดงเป็น 0 โดยใช้ otherwise\n",
    "csv_a.select(csv_a.name, F.when(csv_a.age > 18, 1).when(df.age < 40, -1).otherwise(0)).show()\n",
    "\n",
    "## การเลือกข้อมูลโดยใช้ Like\n",
    "# Like เป็นคำสั่งเมื่อเราต้องการเทียบ หรือจับคู่เหมือน ยกตัวอย่างเช่นการหาคนชื่อซ้ำ หรือหาคนนามสกุลเดียวกัน\n",
    "csv_a.select(\"firstName\", csv_a.lastName.like(\"Smith\")).show()        # ให้แสดง ชื่อ แล้วก็นามสกุล สำหรับคนที่มีนามสกุล Smith\n",
    "\n",
    "## การเลือกข้อมูลโดยใช้ Startswith หรือ Endwith\n",
    "# ใช้ในกรณีที่ต้องการจับคู่คำขึ้นต้นหรือ คำลงท้าย\n",
    "csv_a.select(\"firstName\", csv_a.lastName.startswith(\"Sm\")).show()       # การหาคนที่มีนามสกุลขึ้นต้นด้วย ‘Sm’\n",
    "csv_a.select(csv_a.lastName.endswith(\"th\")).show()                      # หาคนที่มีนามสกุลลงท้ายด้วย ‘th’ \n",
    "\n",
    "## การใช้ Substring\n",
    "# เป็นคำสั่งที่ไว้ใช้เลือกดาต้าในส่วนย่อยเข้าไปอีก โดยเลือกคอลัมน์ที่อยากให้แสดง ตามด้วยฟังชั่น substr แล้วระบุจำนวนตัวอักษรว่าให้เริ่มจากตัวที่เท่าไหร่ จบที่ตัวที่เท่าไหร่ โดยนับจากตัวแรก\n",
    "csv_a.select(csv_a.firstName.substr(1, 3).alias(\"name\")).collect()[Row(name='And'), Row(name='Mic')]        # จากตัวที่หนึ่งถึงตัวที่สาม\n",
    "# alias = มันสามารถเอาไว้เปลี่ยนชื่อคอลัมน์ตอนแสดงออกมาได้ จาก first name เป็น name \n",
    "\n",
    "## การใช้ Between\n",
    "# เป็นการเลือกข้อมูลในช่วงนับตั้งแต่ขอบเขตด้านล่าง ขึ้นไปจนถึงขอบเขตด้านบน และจะแสดงผลออกมาเป็น boolean หรือว่า true/false\n",
    "csv_a.select(csv_a.name, csv_a.age.between(2, 4)).show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### วิธีการ เพิ่ม, ลบ, อัพเดท คอลัมน์ใน DataFrame ###\n",
    "# สามารถจัดการ เพิ่ม ลด เปลี่ยนชื่อ คอลัมน์ได้ดังต่อไปนี้\n",
    "\n",
    "## การเพิ่มคอลัมน์\n",
    "# ใช้ withColumn ตามด้วยชื่อคอลัมน์ที่เราจะตั้ง แล้วตามด้วยค่าในคอลัมน์นั้น, สามารถใช้ค่าจากคอลัมน์ที่มีอยู่แล้วได้ ขึ้นอยู่กับลักษณะข้อมูล\n",
    "csv_a.withColumn('age2', csv_a.age + 2)                     # เป็นการสร้างคอลัมน์ชื่อ age2 โดยการเพิ่มอายุจากคอลัมน์ age ไปอีก 2 ปี\n",
    "csv_a.withColumn('city',csv_a.address.city) \\\n",
    " .withColumn('postalCode',csv_a.address.postalCode) \\\n",
    " .withColumn('state',csv_a.address.state)                   # จะเป็นการสร้างคอลัมน์ใหม่ให้กับ city, postal code และ state โดยเลือกมาจากแต่ละส่วนของ address\n",
    "\n",
    "## การลบคอลัมน์\n",
    "# เมื่อต้องการลบคอลัมน์ที่ไม่ต้องการ ใช้ drop แล้วตามด้วยชื่อคอลัมน์\n",
    "csv_a.drop(\"address\", \"phoneNumber\")                    # การดรอป 2 คอลัมน์ address และ phone number\n",
    "csv_a.drop(csv_a.address).drop(csv_a.phoneNumber)       # เราสามารถทำการ ดรอปซ้อนดรอปได้ดังนี้\n",
    "\n",
    "## การอัพเดทคอลัมน์\n",
    "# อยากเปลี่ยนชื่อคอลัมน์ สามารถทำได้โดยใช้ withColumnRenamed ระบุชื่อคอลัมน์ที่ต้องการจะเปลี่ยนตามด้วยชื่อใหม่\n",
    "csv_a = csv_a.withColumnRenamed('telePhoneNumber', 'phoneNumber')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### การใช้ Operation จัดกลุ่ม กรอง เรียง ข้อมูล\n",
    "# วิธีการใช้ operation ต่างๆโดยเราสามารถจัดกลุ่ม กรองดาต้าตามเงื่อนไข หรือแม้แต่เรียงลำดับได้\n",
    "\n",
    "## การจัดกลุ่มโดย GroupBy\n",
    "# เราสามารถจัดกลุ่มดาต้า แล้วหาจำนวนในแต่ละกลุ่ม หรือค่าเฉลี่ยได้\n",
    "csv_a.groupBy(\"age\").count().show()         # เราอยากรู้ว่าคนอายุเท่านี้ มีทั้งหมดกี่คน ก็ทำได้โดยให้ groupBy อายุ แล้วนับจำนวนด้วย count\n",
    "\n",
    "## การจัดกลุ่มโดย GroupBy\n",
    "# เราสามารถกำหนด condition ได้ \n",
    "csv_a.filter(csv_a['age'] > 21).show()      # การเลือกคนทั้งหมดที่มีอายุมากกว่า น้อยกว่า หรือเท่ากับ ตัวเลขหนึ่ง โดยจะแสดงดาต้าเฟรมออกมาเฉพาะแถวที่เข้า condition นั้น\n",
    "\n",
    "## การเรียงลำดับข้อมูลโดยใช้ Sort\n",
    "# การเรียงลำดับข้อมูล จากมากไปน้อย (descending) หรือ จากน้อยไปมาก (ascending)\n",
    "csv_a.sort(csv_a.age.desc())                               # วิธีที่ 1 จากมากไปน้อย\n",
    "csv_a.sort(\"age\", ascending=False)                         # วิธีที่ 2 จากมากไปน้อย โดยให้น้อยไปมาก เป็น false\n",
    "csv_a.orderBy([\"age\",\"city\"],ascending=[0,1])              # วิธีที่ 3 จากมากไปน้อยโดย age, จากน้อยไปมากโดย city\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### การใช้ Operation จัดกลุ่ม กรอง เรียง ข้อมูล\n",
    "\n",
    "## การเปลี่ยนชนิดของข้อมูล\n",
    "# เราสามารถเล่นกับข้อมูลโดยเปลี่ยน ชนิดของดาต้าไปมาได้ ตั้งแต่เปลี่ยนดาต้าเฟรมเป็น RDD หรือ ให้อยู่ในรูปแบบ Pandas นอกจากเปลี่ยนไปแล้วก็ยังเปลี่ยนกลับได้ด้วย\n",
    "rdd1 = csv_a.rdd                                   # เปลี่ยน dataframe เป็น RDD\n",
    "csv_a.toJSON().first()                             # เปลี่ยน dataframe เป็น string RDD\n",
    "csv_a.toPandas()                                   # ทำให้ spark dataframe อยู่ในรูปแบบ pandas dataframe\n",
    "csv_a = spark.createDataFrame(pandas_df)           # ทำให้ pandas dataframe อยู่ในรูปแบบ spark dataframe\n",
    "\n",
    "## การเซฟไฟล์เป็น Parquet files\n",
    "# ถ้าเราอยากเซฟไฟล์เป็น parquet สามารถทำได้โดยใช้ write.save แล้วตามด้วยชื่อ\n",
    "csv_a.write.save(\"newFile.parquet\")\n",
    "\n",
    "## การเซฟไฟล์เป็น JSON\n",
    "# ในกรณีของ json สามารถทำได้โดยใช้ write.save แล้วตามด้วยชื่อ จากนั้นระบุ format เข้าไปด้วย\n",
    "csv_a.write.save(\"newFile.json\",format=\"json\")\n",
    "\n",
    "## หยุดใช้งาน Spark\n",
    "# ในกรณีที่อยากหยุดการทำงานของ Spark\n",
    "spark.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Bonus: integer (nullable = true)\n",
      " |-- Start Date: date (nullable = true)\n",
      " |-- End Date: date (nullable = true)\n",
      " |-- Resignation Reason: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## แสดง schema\n",
    "csv_a.printSchema()\n",
    "\n",
    "# แสดง schema ดูว่าคอลัมน์มีอะไรบ้าง ชนิดอะไร "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## แสดง statistics (count, mean, stddev, min, max)\n",
    "csv_a.describe().show() \n",
    "\n",
    "## ในกรณีที่ต้องการให้แสดงเฉพาะคอลัมน์ก็ใส่ชื่อเข้าไปได้\n",
    "csv_a.describe(['age']).show()\n",
    "\n",
    "# อยากรู้สถิติของข้อมูลว่าแต่ละคอลัมน์เป็นอย่างไร ทำได้โดยใช้ describe() ตามด้วย show() เพื่อแสดงข้อมูล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## การดรอป ค่าซ้ำ  -  เมื่อมีการใส่ค่าผิด อย่างเช่น ใส่อายุผิด โดยที่ชื่อและ ความสูงถูก ทำให้มีแถวที่ซ้ำกันสองแถว เราสามารถดรอปแถวที่ซ้ำได้\n",
    "df.dropDuplicates()\n",
    "\n",
    "## พอเราดรอปค่าซ้ำ\n",
    "df.dropDuplicates().show()\n",
    "\n",
    "## ถ้สต้องการดรอปโดบเจาะจงคอลัมน์\n",
    "df.dropDuplicates(['name', 'height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = sc.textFile(\"record.txt\")\n",
    "output = record.map(lambda _: 1).reduce(lambda a, b: a + b)\n",
    "\n",
    "\n",
    "# โค้ดนี้จะใช้ RDD ของ Spark เพื่อนับจำนวนบรรทัดในไฟล์ record.txt\n",
    "\n",
    "# ฟังก์ชัน map() จะแปลงแต่ละบรรทัดใน RDD เป็นค่า 1 ซึ่งแทนจำนวนบรรทัด\n",
    "# ฟังก์ชัน reduce() จะรวมค่าทั้งหมดใน RDD เข้าด้วยกันโดยใช้ฟังก์ชัน + ดังนั้น ผลลัพธ์ของโค้ดนี้คือจำนวนบรรทัดในไฟล์ record.txt\n",
    "\n",
    "# ตัวอย่างเช่น หากไฟล์ record.txt มี 10 บรรทัด ผลลัพธ์ของโค้ดนี้คือ 10\n",
    "# sc.textFile(\"record.txt\").map(lambda _: 1).reduce(lambda a, b: a + b)\n",
    "# Output: 10\n",
    "\n",
    "# หากไฟล์ record.txt ว่างเปล่า ผลลัพธ์ของโค้ดนี้คือ 0\n",
    "# sc.textFile(\"record.txt\").map(lambda _: 1).reduce(lambda a, b: a + b)\n",
    "# Output: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ต้องการเลือกคอลัมน์ age และ salary ของ DataFrame df สามารถทำได้ 2 วิธีดังนี้\n",
    "\n",
    "# วิธีแรก ใช้ฟังก์ชัน iloc()\n",
    "# df.iloc[:, [0, 1]]\n",
    "\n",
    "# ฟังก์ชัน iloc() จะรับอาร์กิวเมนต์เป็นสองช่วง เช่น [start:end] หรือ [start:end, start:end] ในกรณีนี้ อาร์กิวเมนต์แรกคือช่วงของแถว ซึ่งใช้ : เพื่อระบุช่วงทั้งหมด อาร์กิวเมนต์ที่สองคือช่วงของคอลัมน์ ซึ่งใช้ [0, 1] เพื่อระบุคอลัมน์ age และ salary\n",
    "\n",
    "# ดังนั้น ผลลัพธ์ของโค้ดข้างต้นคือ\n",
    "\n",
    "# age  salary\n",
    "# 0    25     30000\n",
    "# 1    30     40000\n",
    "# 2    35     50000\n",
    "# 3    40     60000\n",
    "# 4    45     70000\n",
    "\n",
    "# วิธีที่สอง ใช้ฟังก์ชัน loc()\n",
    "# df[['age', 'salary']]\n",
    "\n",
    "# ฟังก์ชัน loc() จะรับอาร์กิวเมนต์เป็น list ของชื่อคอลัมน์\n",
    "\n",
    "# ดังนั้น ผลลัพธ์ของโค้ดข้างต้นคือ\n",
    "\n",
    "# age  salary\n",
    "# 0    25     30000\n",
    "# 1    30     40000\n",
    "# 2    35     50000\n",
    "# 3    40     60000\n",
    "# 4    45     70000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import namedtuple\n",
    "# from pathlib import Path\n",
    "# from typing import List\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # List of all tables used in the original database\n",
    "# TABLES = [\n",
    "#     \"addresses\",\n",
    "#     \"birthdates\",\n",
    "#     \"cities\",\n",
    "#     \"countries\",\n",
    "#     \"cuisines\",\n",
    "#     \"districts\",\n",
    "#     \"food\",\n",
    "#     \"orders\",\n",
    "#     \"promos\",\n",
    "#     \"restaurants\",\n",
    "#     \"states\",\n",
    "#     \"users\",\n",
    "# ]\n",
    "\n",
    "# # Path to the directory where tables' CSV files are stored\n",
    "# TABLES_DIR_PATH = Path(__file__).parent / \"tables\"\n",
    "\n",
    "# # Structure holding initial database\n",
    "# MultiDimDatabase = namedtuple(\n",
    "#     \"MultiDimDatabase\",\n",
    "#     [\n",
    "#         \"addresses\",\n",
    "#         \"birthdates\",\n",
    "#         \"cities\",\n",
    "#         \"countries\",\n",
    "#         \"cuisines\",\n",
    "#         \"districts\",\n",
    "#         \"food\",\n",
    "#         \"orders\",\n",
    "#         \"promos\",\n",
    "#         \"restaurants\",\n",
    "#         \"states\",\n",
    "#         \"users\",\n",
    "#     ],\n",
    "# )\n",
    "# ReducedDatabase = namedtuple(\n",
    "#     \"ReducedDatabase\", [\"orders\", \"users\", \"food\", \"promos\", \"restaurants\", \"addresses\"]\n",
    "# )\n",
    "\n",
    "\n",
    "# # --- Task #1 ---\n",
    "# def load_tables(tables_dir_path: Path, tables: List[str]) -> List[pd.DataFrame]:\n",
    "#   \"\"\"\n",
    "# function reads tables from a CSV file\n",
    "\n",
    "# It takes two parameters: tables_dir_path and tables \n",
    "#      1. Verify that tables_dir_path is the Path object to the directory that contains the CSV file\n",
    "#      2. Verify that tables is a list of table names that we need to load\n",
    "#      3. For each CSV file in the tables_dir_path directory\n",
    "#         - Create a DataFrame from the CSV file and name the DataFrame after the table name specified in tables\n",
    "#      4. Returns a list of created DataFrames\n",
    "\n",
    "#   Returns : list of DataFrames\n",
    "#   \"\"\"\n",
    "#   if not isinstance(tables_dir_path, Path):\n",
    "#     raise ValueError(\"tables_dir_path must be a Path object\")\n",
    "\n",
    "#   if not isinstance(tables, list):\n",
    "#     raise ValueError(\"tables must be a list of strings\")\n",
    "\n",
    "#   tables_data = []\n",
    "#   for table in tables:\n",
    "#     table_path = tables_dir_path / f\"{table}.csv\"\n",
    "#     if not table_path.is_file():\n",
    "#       raise ValueError(f\"File '{table}.csv' does not exist\")\n",
    "\n",
    "#     df = pd.read_csv(table_path)\n",
    "#     df.columns = df.columns.str.lower()\n",
    "#     tables_data.append(df)\n",
    "\n",
    "#   return tables_data\n",
    "\n",
    "#     # raise NotImplementedError()\n",
    "\n",
    "\n",
    "# # --- Task # 2 ---\n",
    "# def reduce_dims(db: MultiDimDatabase) -> ReducedDatabase:\n",
    "#   \"\"\"\n",
    "# Transform the database and reduce its dimensionality\n",
    "# Returns : ReducedDatabase\n",
    "#   \"\"\"\n",
    "#   reduced_db = ReducedDatabase(\n",
    "#       orders=None,\n",
    "#       users=None,\n",
    "#       food=None,\n",
    "#       promos=None,\n",
    "#       restaurants=None,\n",
    "#       addresses=None,\n",
    "#   )\n",
    "\n",
    "#   reduced_db.orders = _transform_orders(db.orders)\n",
    "#   reduced_db.users = _transform_users(db.users)\n",
    "#   reduced_db.food = _transform_food(db.food)\n",
    "#   reduced_db.promos = _transform_promos(db.promos)\n",
    "#   reduced_db.restaurants = _transform_restaurants(db.restaurants, db.food)\n",
    "#   reduced_db.addresses = _transform_addresses(db.addresses)\n",
    "\n",
    "#   return reduced_db\n",
    "\n",
    "\n",
    "# def _transform_orders(orders: pd.DataFrame) -> pd.DataFrame:\n",
    "#   keep_columns = [\"order_id\", \"customer_id\", \"restaurant_id\", \"created_at\", \"delivered_at\", \"total_price\"]\n",
    "#   return orders[keep_columns].copy()\n",
    "\n",
    "# def _transform_users(users: pd.DataFrame) -> pd.DataFrame:\n",
    "#   keep_columns = [\"user_id\", \"name\", \"email\", \"created_at\"]\n",
    "#   return users[keep_columns].copy()\n",
    "\n",
    "# def _transform_food(food: pd.DataFrame) -> pd.DataFrame:\n",
    "#   keep_columns = [\"food_id\", \"name\", \"price\"]\n",
    "#   return food[keep_columns].copy()\n",
    "\n",
    "# def _transform_promos(promos: pd.DataFrame) -> pd.DataFrame:\n",
    "#   keep_columns = [\"promo_id\", \"name\", \"discount\", \"expiry_date\"]\n",
    "#   return promos[keep_columns].copy()\n",
    "\n",
    "# def _transform_restaurants(restaurants: pd.DataFrame, food: pd.DataFrame) -> pd.DataFrame:\n",
    "#   keep_columns = [\"restaurant_id\", \"name\", \"address\", \"cuisine_id\"]\n",
    "\n",
    "#   restaurants[\"most_popular_food\"] = food[food[\"restaurant_id\"] == restaurants[\"restaurant_id\"]][\"name\"].mode(dropna=False)\n",
    "#   return restaurants[keep_columns].copy()\n",
    "\n",
    "# def _transform_addresses(addresses: pd.DataFrame) -> pd.DataFrame:\n",
    "#   keep_columns = [\"address_id\", \"street\", \"city\", \"state\", \"country\"]\n",
    "\n",
    "#   addresses[\"full_address\"] = addresses[[\"street\", \"city\", \"state\", \"country\"]].apply(lambda row: \", \".join(row.dropna()), axis=1)\n",
    "#   return addresses[keep_columns].copy()\n",
    "\n",
    "#     # raise NotImplementedError()\n",
    "\n",
    "\n",
    "# # --- Task #3 ---\n",
    "# def create_orders_by_meal_type_age_cuisine_table(db: ReducedDatabase) -> pd.DataFrame:\n",
    "#   \"\"\"\n",
    "#   Creates a new table named \"orders_by_meal_type_age_cuisine\" from the provided database\n",
    "#   \"\"\"\n",
    "#   required_tables = [\"orders\", \"users\", \"food\"]\n",
    "#   for table in required_tables:\n",
    "#     if not getattr(db, table):\n",
    "#       raise ValueError(f\"Table '{table}' is missing from the database\")\n",
    "\n",
    "#   orders_df = pd.merge(\n",
    "#       db.orders, db.users, on=[\"user_id\", \"order_id\"], how=\"inner\"\n",
    "#   )\n",
    "\n",
    "#   orders_df[\"meal_type\"] = orders_df[\"order_time\"].apply(\n",
    "#       lambda x: \"breakfast\" if x.hour < 10 else \"lunch\" if x.hour < 16 else \"dinner\"\n",
    "#   )\n",
    "\n",
    "#   orders_df[\"user_age\"] = orders_df[\"birthdate\"].apply(\n",
    "#       lambda x: \"young\" if x.year >= 1995 else \"adult\" if x.year >= 1970 else \"old\"\n",
    "#   )\n",
    "\n",
    "#   orders_df[\"food_cuisine\"] = orders_df[\"food_id\"].apply(db.food[\"cuisine\"])\n",
    "\n",
    "#   return orders_df[\n",
    "#       [\"order_id\", \"meal_type\", \"user_age\", \"food_cuisine\"]\n",
    "#   ].set_index(\"order_id\")\n",
    "\n",
    "#     # raise NotImplementedError()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
